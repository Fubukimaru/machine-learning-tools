% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wrapper.R
\name{train.mlp}
\alias{train.mlp}
\title{Training a MultiLayer Perceptron Neural Network Function}
\usage{
train.mlp(dataset, targets, layers, batch_size = 10, training_epochs = 10,
  learning_rate = 0.001, momentum = 0.8, rand_seed = 1234)
}
\arguments{
\item{dataset}{A matrix with data, one example per row.}

\item{targets}{A matrix with output labels, one set of targets per row.}

\item{batch_size}{Number of examples per training mini-batch. Default = 1.}

\item{training_epochs}{Number of training epochs. Default = 1000.}

\item{learning_rate}{The learning rate for training. Default = 0.01.}

\item{momentum}{The momentum for training. Default = 0.8. (Not Implemented!)}

\item{rand_seed}{Random seed. Default = 1234.}
}
\description{
This function trains a MLP ANN. Admits as parameters the training dataset,
the matrix of outputs, and a descriptor of the network including all the
layers and their properties. Returns a MLP in list form, including all the
trained layers.
Possible layers are:
\itemize{
  \item LINE: Linear Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Number of hidden units
    \item Scale for initialization weights
    \item Batch_size
  }
  \item RELV: Rectified Linear (for flattened batches). Requires, in the following order:
  \enumerate{
    \item Batch_size
  }
  \item SOFT: SoftMax Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
  \item SIGM: Sigmoid Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
  \item TANH: Hyperbolic Tangent Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
  \item DIRE: Direct (buffer) Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
}
The attribute "batch_size" must be the same for all layers. The list of
layers is checked before the process starts.
}
\examples{
## Simple example
train_X <- array(c(1, 1, 1, 0, 0, 0,
                   1, 0, 1, 0, 0, 0,
                   1, 1, 1, 0, 0, 0,
                   0, 0, 1, 1, 1, 0,
                   0, 0, 1, 0, 1, 0,
                   0, 0, 1, 1, 1, 0),
                   c(6, 6));
train_Y <- array(c(1, 0,
                   1, 0,
                   1, 1,
                   0, 0,
                   0, 1,
                   0, 1), c(6, 2));

batch_size <- 2;

layers <- list(
   c("LINE", 6, 2, 0.1, batch_size),
   c("RELU", 2, batch_size),
   c("SOFT", 2, batch_size)
);
mlp1 <- train.mlp(train_X, train_Y, layers);

## The MNIST example
data(mnist)

train <- mnist$train;
training_x <- train$x / 255;
training_y <- binarization(train$y);

test <- mnist$test;
testing_x <- test$x / 255;
testing_y <- binarization(test$y);

dataset <- training_x[1:1000,, drop=FALSE];
targets <- training_y[1:1000,, drop=FALSE];

newdata <- testing_x[1:1000,, drop=FALSE];

batch_size <- 10;
training_epochs <- 10;
learning_rate <- 1e-3;
momentum <- 0.8;
rand_seed <- 1234;

layers <- list(
             c("LINE", 784, 64, 0.1, batch_size),
             c("RELV", batch_size),
             c("LINE", 64, 10, 0.1, batch_size),
             c("SOFT", 10, batch_size)
);

mnist_mlp <- train.mlp(dataset, targets, layers, batch_size, training_epochs,
                       learning_rate, momentum, rand_seed);
}
\keyword{MLP}
