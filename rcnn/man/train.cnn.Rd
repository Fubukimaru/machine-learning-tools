% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wrapper.R
\name{train.cnn}
\alias{train.cnn}
\title{Training a Convolutional Neural Network Function}
\usage{
train.cnn(dataset, targets, layers, batch_size = 10, training_epochs = 10,
  learning_rate = 0.001, momentum = 0.8, rand_seed = 1234)
}
\arguments{
\item{dataset}{A matrix with data, one example per row.}

\item{targets}{A matrix with output labels, one set of targets per row.}

\item{batch_size}{Number of examples per training mini-batch. Default = 1.}

\item{training_epochs}{Number of training epochs. Default = 1000.}

\item{learning_rate}{The learning rate for training. Default = 0.01.}

\item{momentum}{The momentum for training. Default = 0.8. (Not Implemented!)}

\item{rand_seed}{Random seed. Default = 1234.}
}
\description{
This function trains a CNN. Admits as parameters the training dataset, the
matrix of outputs, and a descriptor of the network including all the layers
and their properties. Returns a CNN in list form, including all the trained
layers.
Possible layers are:
\itemize{
  \item CONV: Convolutional Layer. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image
    \item Number of filters to be returned
    \item Size of convolutional filters
    \item Scale for initialization weights
    \item Border Mode (1 = valid, 2 = same, 3 = full)
    \item Batch_size
  }
  \item POOL: Pooling Layer. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image.
    \item Scale for initialization weights
    \item Batch_size
    \item Window size
    \item Stride
  }
  \item RELU: Rectified Linear. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image.
    \item Batch_size
  }
  \item FLAT: Flattening Layer. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image.
    \item Batch_size
  }
  \item LINE: Linear Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Number of hidden units
    \item Scale for initialization weights
    \item Batch_size
  }
  \item RELV: Rectified Linear (for flattened batches). Requires, in the
  following order:
  \enumerate{
    \item Batch_size
  }
  \item SOFT: SoftMax Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
  \item SIGM: Sigmoid Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
  \item TANH: Hyperbolic Tangent Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
  \item DIRE: Direct (buffer) Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Batch_size
  }
}
The attribute "batch_size" must be the same for all layers. The list of
layers is checked before the process starts.
}
\examples{
train_X <- t(array(c(1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
                     0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0),
                     c(6, 1, 2, 3)));
train_Y <- t(array(c(1, 0,
                     1, 0,
                     1, 1,
                     0, 0,
                     0, 1,
                     0, 1), c(6, 2)));

batch_size <- 2;
filter_size <- 5;
border_mode <- 2; # "VALID" mode
win_size <- 3;
stride <- 2;

layers <- list(
   c("CONV", 1, 4, filter_size, 0.1, border_mode, batch_size),
   c("POOL", 4, 0.1, batch_size, win_size, stride),
   c("RELU", 4, batch_size),
   c("FLAT", 4, batch_size),
   c("LINE", 6, 2, 0.1, batch_size),
   c("SOFT", 2, batch_size)
);
cnn1 <- train.cnn(train_X, train_Y, layers);
}
\keyword{CNN}
