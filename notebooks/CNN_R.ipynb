{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in R\n",
    "\n",
    "### CNN training and testing algorithms, coded in plain R.\n",
    "\n",
    "Here you can find a Convolutional Neural Networks, implemented in R and in plain algorithm, for academic and educational purposes.\n",
    "\n",
    "@author Josep Ll. Berral (Barcelona Supercomputing Center)\n",
    "\n",
    "@date 3rd March 2017\n",
    "\n",
    "\n",
    "### References:\n",
    "* Approach based on Lars Maaloee's: https://github.com/davidbp/day2-Conv\n",
    "* Also from LeNet (deeplearning.net): http://deeplearning.net/tutorial/lenet.html\n",
    "\n",
    "### Mocap data:\n",
    "* The MNIST digit recognition dataset http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERIC FUNCTIONS\n",
    "\n",
    "* **sample_normal**: Generates a matrix of random normal values\n",
    "* **sample_bernoulli**: Generates a matrix of Bernoulli samples given a matrix of probabilities\n",
    "* **sigmoid_func**: Performs the sigmoid calculus over a matrix\n",
    "* **%+%**: Operator to sum a vector to a matrix by their coincident side (row checked before columm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function to produce Normal Samples\n",
    "sample_normal <- function(dims, mean = 0, sd = 1)\n",
    "{\n",
    "    array(rnorm(n = prod(dims), mean = mean, sd = sd), dims);\n",
    "}\n",
    "\n",
    "## Function to produce Bernoulli Samples\n",
    "sample_bernoulli <- function(mat)\n",
    "{\n",
    "    dims <- dim(mat);\n",
    "    array(rbinom(n = prod(dims), size = 1, prob = c(mat)), dims);\n",
    "}\n",
    "\n",
    "## Function to produce the Sigmoid\n",
    "sigmoid_func <- function(mat)\n",
    "{\n",
    "    1 / (1 + exp(-mat));\n",
    "}\n",
    "\n",
    "## Operator to add dimension-wise vectors to matrices\n",
    "`%+%` <- function(mat, vec)\n",
    "{\n",
    "    retval <- NULL;\n",
    "    tryCatch(\n",
    "        expr = { retval <- if (dim(mat)[1] == length(vec)) t(t(mat) + vec) else mat + vec; },\n",
    "        warning = function(w) { print(paste(\"WARNING: \", w, sep = \"\")); },\n",
    "        error = function(e) { print(paste(\"ERROR: Cannot sum mat and vec\", e, sep = \"\\n\")); }\n",
    "    );\n",
    "    retval;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **binarization**: Produces a matrix of binarized outputs from a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to produce binarized outputs from Vector\n",
    "binarization <- function(vec)\n",
    "{\n",
    "    result <- array(0, c(length(vec),length(unique(vec))));\n",
    "    for (i in 1:length(vec)) result[i,vec[i]] <- 1;\n",
    "    result;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and Image Functions\n",
    "\n",
    "Here we implement the Convolution and Image Padding functions\n",
    "* **conv2D**: performs the convolution of an image matrix and a filter matrix.\n",
    "* **img\\_padding**: adds padding to an image matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convolution - Version in \"Native R\"\n",
    "## Performs the Convolution of mat (4D) using filter k (1,f,1,1)\n",
    "conv2D <- function(mat, k, mode = 'valid')\n",
    "{\n",
    "    out <- conv2D_sub(mat, k);\n",
    "    krow <- nrow(k);\n",
    "    kcol <- ncol(k);\n",
    "\n",
    "    krow_h <- krow %/% 2;\n",
    "    kcol_h <- kcol %/% 2;\n",
    "\n",
    "    mrow <- nrow(mat);\n",
    "    mcol <- ncol(mat);\n",
    "\n",
    "    out <- array(0, c(mrow,mcol));\n",
    "    for(i in 1:mrow)\n",
    "    {\n",
    "        for(j in 1:mcol)\n",
    "        {\n",
    "            acc <- 0;\n",
    "            for(m in 1:krow)\n",
    "            {\n",
    "                mm <- krow - m + 1;\n",
    "                ii <- i + m - krow_h - 1;\n",
    "                for(n in 1:kcol)\n",
    "                {\n",
    "                    nn <- kcol - n + 1;\n",
    "                    jj <- j + n - kcol_h - 1;\n",
    "\n",
    "                    if( ii > 0 && ii <= mrow && jj > 0 && jj <= mcol)\n",
    "                        acc <- acc + mat[ii,jj] * k[mm,nn];\n",
    "                }\n",
    "            }\n",
    "            out[i,j] <- acc;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (mode == 'valid')\n",
    "    {\n",
    "        cut_y <- krow_h + 1;\n",
    "        cut_x <- kcol_h + 1;\n",
    "\n",
    "        len_y <- max(krow,mrow) - min(krow,mrow) + 1;\n",
    "        len_x <- max(kcol,mcol) - min(kcol,mcol) + 1;\n",
    "\n",
    "        out <- out[cut_y:(cut_y + len_y - 1),cut_x:(cut_x + len_x - 1)];\n",
    "    }\n",
    "\n",
    "    out;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Image Padding - Version in \"Native R\"\n",
    "img_padding <- function(img, pad_y, pad_x)\n",
    "{\n",
    "    dims <- dim(img);\n",
    "    imgs_pad <- array(0, c(dims[1] + 2 * pad_y, dims[2] + 2 * pad_x));\n",
    "\n",
    "    aux <- cbind(img, array(0, c(nrow(img), pad_y)));\n",
    "    aux <- cbind(array(0, c(nrow(aux), pad_y)), aux);\n",
    "    aux <- rbind(aux, array(0, c(pad_x, ncol(aux))));\n",
    "    aux <- rbind(array(0, c(pad_x, ncol(aux))), aux);\n",
    "    aux;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then we notice that such functions can be really slow, and cannot be fully parallelized. We have the option of using Rcpp and coding them in C++-like language, betraying a little the title and subtitle of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(\"Rcpp\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Performs the Convolution of mat (4D) using filter k (1,f,1,1)\n",
    "cppFunction('\n",
    "NumericMatrix conv2D(NumericMatrix mat, NumericMatrix k, String mode = \"valid\")\n",
    "{\n",
    "    int krow = k.nrow();\n",
    "    int kcol = k.ncol();\n",
    "\n",
    "    int krow_h = krow / 2;\n",
    "    int kcol_h = kcol / 2;\n",
    "\n",
    "    int mrow = mat.nrow();\n",
    "    int mcol = mat.ncol();\n",
    "\n",
    "    NumericMatrix out(mrow, mcol);\n",
    "\n",
    "    for(int i = 0; i < mrow; ++i)\n",
    "    {\n",
    "        for(int j = 0; j < mcol; ++j)\n",
    "        {\n",
    "            double acc = 0;\n",
    "            for(int m = 0; m < krow; ++m)\n",
    "            {\n",
    "                int mm = krow - 1 - m;\n",
    "                int ii = i + (m - krow_h);\n",
    "\n",
    "                if (ii >= 0 && ii < mrow)\n",
    "                    for(int n = 0; n < kcol; ++n)\n",
    "                    {\n",
    "                        int nn = kcol - 1 - n;\n",
    "                        int jj = j + (n - kcol_h);\n",
    "\n",
    "                        if (jj >= 0 && jj < mcol) acc += mat(ii,jj) * k(mm,nn);\n",
    "                    }\n",
    "            }\n",
    "            out(i,j) = acc;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (mode == \"valid\")\n",
    "    {\n",
    "        int cut_y = krow_h;\n",
    "        int cut_x = kcol_h;\n",
    "\n",
    "        int len_y = std::max(krow,mrow) - std::min(krow,mrow);// + 1;\n",
    "        int len_x = std::max(kcol,mcol) - std::min(kcol,mcol);// + 1;\n",
    "        out = out(Rcpp::Range(cut_y, cut_y + len_y), Rcpp::Range(cut_x, cut_x + len_x));\n",
    "    }\n",
    "\n",
    "    return out;\n",
    "}\n",
    "')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Functions\n",
    "\n",
    "First of all, we load a few functions that will allow us to check each implemented layer, looking if gradients are well computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Functions for checking gradient correctness\n",
    "source(\"../grad_check.R\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the different layers we will use (e.g. convolutional, pooling, relu...). For this, we will understant each layer as an \"object\" containing functions like \"forward\", \"backward\", \"get_updates\", \"create\", etc.\n",
    "\n",
    "Then, each kind of layer will \"inherit\" from this _layer object_, so their \"create_xxx\" function will return a list with its attributes, and its \"forward_xxx\", \"backward_xxx\", ... functions as \"forward\", \"backward\", and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "Functions for creating and operating Convolutional layers:\n",
    "* **conv_bc01**: Prepares an image matrix and performs its convolution against a filter\n",
    "* **forward_conv**: Forward function. Activation of outputs given inputs\n",
    "* **backward_conv**: Backward function. Teconstruction of inputs given outputs\n",
    "* **get_updates_conv**: Update of weights in hidden units and bias\n",
    "\n",
    "* **pnames_conv**: Returns the names of the elements in the layer\n",
    "* **gnames_conv**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_conv**: The constructor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that such operations will also be slow as heck, so we are using some \"pre-compilation\" by passing our functions through *cmpfun* from R *compiler* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(\"compiler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Performs the convolution\n",
    "##   param imgs: <batch_size, img_n_channels, img_height, img_width>\n",
    "##   param filters: <n_filters, n_channels, win_height, win_width>\n",
    "##   param padding: <padding_y, padding_x>\n",
    "conv_bc01_orig <- function(imgs, filters, padding)\n",
    "{\n",
    "    # Compute shapes\n",
    "    imgs.shape <- dim(imgs);\n",
    "    batch_size <- imgs.shape[1];\n",
    "    n_channels_img <- imgs.shape[2];\n",
    "    img_h <- imgs.shape[3];\n",
    "    img_w <- imgs.shape[4];\n",
    "\n",
    "    filters.shape <- dim(filters);\n",
    "    n_filters <- filters.shape[1];\n",
    "    n_channels <- filters.shape[2];\n",
    "    win_h <- filters.shape[3];\n",
    "    win_w <- filters.shape[4];\n",
    "\n",
    "    pad_y <- padding[1];\n",
    "    pad_x <- padding[2];\n",
    "\n",
    "    if (!(n_channels == n_channels_img))\n",
    "    {\n",
    "        warning('Mismatch in # of channels');\n",
    "        return(NULL);\n",
    "    }\n",
    "\n",
    "    # Create output array\n",
    "    out_h <- (img_h - win_h + 2 * pad_y) + 1;\n",
    "    out_w <- (img_w - win_w + 2 * pad_x) + 1;\n",
    "    out_shape <- c(batch_size, n_filters, out_h, out_w);\n",
    "    out_1 <- array(0, out_shape);\n",
    "\n",
    "    # Prepares padded image for convolution\n",
    "    imgs_pad <- array(0, dim(imgs) + c(0, 0, 2*(pad_y), 2*(pad_x)));\n",
    "    for (i in 1:dim(imgs)[1])\n",
    "        for (j in 1:dim(imgs)[2])\n",
    "            imgs_pad[i,j,,] <- img_padding(imgs[i,j,,], pad_y, pad_x);\n",
    "\n",
    "    # Perform convolution\n",
    "    for (b in 1:batch_size)\n",
    "        for (f in 1:n_filters)\n",
    "            for (c in 1:n_channels)\tout_1[b,f,,] <- out_1[b,f,,] + conv2D(imgs_pad[b,c,,], filters[f,c,,]);\n",
    "\n",
    "    return(out_1);\n",
    "}\n",
    "conv_bc01 <- cmpfun(conv_bc01_orig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Performs Forward Propagation\n",
    "##   param x : Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "##   return  : Array of shape (batch_size, n_filters, out_height, out_width)\n",
    "##   updates : conv_layer\n",
    "forward_conv_orig <- function(conv, x)\n",
    "{      \n",
    "    # Save \"x\" for back-propagation\n",
    "    conv[[\"x\"]] <- x;\n",
    "\n",
    "    # Performs convolution\n",
    "    y <- conv_bc01(x, conv$W, conv$padding);\n",
    "\n",
    "    for (b in 1:dim(y)[1]) y[b,,,] <- y[b,,,] + conv$b[1,,1,1];\n",
    "\n",
    "    list(layer = conv, y = y);\n",
    "}\n",
    "forward_conv <- cmpfun(forward_conv_orig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Performs Backward Propagation\n",
    "##   param dy : Array of shape (batch_size, n_filters, out_height, out_width)\n",
    "##   return   : Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "##   updates  : conv_layer\n",
    "backward_conv_orig <- function(conv, dy)\n",
    "{\n",
    "    # Flip weights\n",
    "    w <- conv$W[,, (conv$w_shape[3]:1), (conv$w_shape[4]:1), drop = FALSE];\n",
    "\n",
    "    # Transpose channel/filter dimensions of weights\n",
    "    w <- aperm(w, c(2, 1, 3, 4));\n",
    "\n",
    "    # Propagate gradients to x\n",
    "    dx <- conv_bc01(dy, w, conv$padding);\n",
    "\n",
    "    # Prepares padded image for convolution\n",
    "    x_pad <- array(0, dim(conv$x) + c(0, 0, 2 * conv$padding));\n",
    "    for (i in 1:dim(x_pad)[1])\n",
    "        for (j in 1:dim(x_pad)[2])\n",
    "            x_pad[i,j,,] <- img_padding(conv$x[i,j,,], conv$padding[1], conv$padding[2]);\n",
    "\n",
    "    # Propagate gradients to weights and gradients to bias\n",
    "    grad_W <- array(0, dim(conv$W));\n",
    "    for (b in 1:dim(dy)[1])\n",
    "        for (f in 1:dim(conv$W)[1])\n",
    "            for (c in 1:dim(conv$W)[2])\n",
    "                grad_W[f,c,,] <- grad_W[f,c,,] + conv2D(x_pad[b,c,,], dy[b,f,,]);\n",
    "\n",
    "    conv[[\"grad_W\"]] <- grad_W[,, (conv$w_shape[3]:1), (conv$w_shape[4]:1), drop = FALSE];\n",
    "    conv[[\"grad_b\"]] <- array(apply(dy, MARGIN = 2, sum), dim(conv$b));\n",
    "\n",
    "    list(layer = conv, dx = dx);\n",
    "}\n",
    "backward_conv <- cmpfun(backward_conv_orig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Updates the Convolutional Layer\n",
    "get_updates_conv_orig <- function(conv, lr)\n",
    "{\n",
    "    conv[[\"W\"]] = conv$W - conv$grad_W * lr;\n",
    "    conv[[\"b\"]] = conv$b - conv$grad_b * lr;\n",
    "    conv;\n",
    "}\n",
    "get_updates_conv <- cmpfun(get_updates_conv_orig);\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_conv <- function(conv) { c(\"W\",\"b\"); }\n",
    "gnames_conv <- function(conv) { c(\"grad_W\",\"grad_b\"); }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a convolutional layer\n",
    "create_conv <- function(n_channels, n_filters, filter_size, scale = 0.01, border_mode = 'same')\n",
    "{\n",
    "    dims <- c(n_filters, n_channels, filter_size, filter_size);\n",
    "\n",
    "    W <- scale * sample_normal(dims);\n",
    "    b <- array(0, c(1, n_filters, 1 ,1));\n",
    "\n",
    "    if (border_mode == 'valid') padding <- 0;\n",
    "    if (border_mode == 'same') padding <- filter_size %/% 2;\n",
    "    if (border_mode == 'full') padding <- filter_size - 1;\n",
    "\n",
    "    padding <- c(padding, padding);\n",
    "\n",
    "    list(n_channels = n_channels, n_filters = n_filters, filter_size = filter_size,\n",
    "        w_shape = dims,\tW = W, b = b, padding = padding, pnames = pnames_conv, gnames = gnames_conv,\n",
    "        forward = forward_conv, backward = backward_conv, get_updates = get_updates_conv);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a simplistic *Driver*, to check whether the layer is well encoded and computes the gradient properly. Also it will work as an use example for that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_conv <- function()\n",
    "{\n",
    "    batch_size <- 10;\n",
    "    n_channels <- 1;\n",
    "    img_shape <- c(5, 5);\n",
    "    n_filters <- 2;\n",
    "    filter_size <- 3;\n",
    "\n",
    "    border_mode <- 'same';\n",
    "\n",
    "    x <- sample_normal(c(batch_size, n_channels, img_shape));\n",
    "    layer <- create_conv(n_channels, n_filters, filter_size, border_mode = border_mode);\n",
    "\n",
    "    ok <- check_grad(layer, x);\n",
    "    if (ok)\tprint('Gradient check passed') else print('Gradient check failed');\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "Functions for creating and operating Pooling layers:\n",
    "* **forward_pool**: Forward function. TODO - Explanation\n",
    "* **backward_pool**: Backward function. TODO - Explanation\n",
    "* **get_updates_pool**: Pooling layer does not need to update weights and biases, so it does nothing.\n",
    "\n",
    "* **pnames_pool**: Returns the names of the elements in the layer\n",
    "* **gnames_pool**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_pool**: The constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forwards a Pooling Matrix (4D) from a Convolutional Matrix (4D)\n",
    "##   param x : Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "##   returns : Array of shape (batch_size, n_channels, out_height, out_width)\n",
    "##   updates : pool_layer\n",
    "forward_pool_orig <- function(pool, imgs)\n",
    "{\n",
    "    # Compute shapes\n",
    "    imgs.shape <- dim(imgs);\n",
    "    batch_size <- imgs.shape[1];\n",
    "    n_channels <- imgs.shape[2];\n",
    "    img_h <- imgs.shape[3];\n",
    "    img_w <- imgs.shape[4];\n",
    "\n",
    "    # Store x for brop()\n",
    "    pool[[\"imgs\"]] <- imgs;\n",
    "\n",
    "    # Create output array\n",
    "    out_h <- (img_h - pool$win_size + 2 * pool$padding) %/% pool$stride + 1;\n",
    "    out_w <- (img_w - pool$win_size + 2 * pool$padding) %/% pool$stride + 1;\n",
    "    out <- array(0, c(batch_size, n_channels, out_h, out_w));\n",
    "\n",
    "    # Perform average pooling\n",
    "    imgs <- imgs / (pool$win_size)^2;\n",
    "    for (b in 1:batch_size)\n",
    "        for (c in 1:n_channels)\n",
    "            for (y in 1:out_h)\n",
    "            {\n",
    "                yaux <- y * pool$stride - 1;\n",
    "                pa <- max(yaux,1):min((yaux + pool$win_size - 1), img_h);\n",
    "                for (x in 1:out_w)\n",
    "                {\n",
    "                    xaux <- x * pool$stride - 1;\n",
    "                    pb <- max(xaux,1):min((xaux + pool$win_size - 1), img_w);\n",
    "                    out[b, c, y, x] <- sum(imgs[b, c, pa, pb]);\n",
    "                }\n",
    "            }\n",
    "\n",
    "    list(layer = pool, y = out);\n",
    "}\n",
    "forward_pool <- cmpfun(forward_pool_orig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Backwards a Pooling Matrix (4D) to a Convolutional Matrix (4D)\n",
    "##   param dy : Array of shape (batch_size, n_channels, out_height, out_width)\n",
    "##   return   : Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "##   updates  : pool_layer\n",
    "backward_pool_orig <- function(pool, dy)\n",
    "{\n",
    "    dx <- array(0, dim(pool$imgs));\n",
    "    dy <- dy / pool$win_size^2;\n",
    "\n",
    "    dx_h <- dim(dx)[3];\n",
    "    dx_w <- dim(dx)[4];\n",
    "\n",
    "    for (i in 1:(dim(dx)[1]))\n",
    "        for (c in 1:(dim(dx)[2]))\n",
    "            for (y in 1:(dim(dy)[3]))\n",
    "            {\n",
    "                yaux <- y * pool$stride - 1;\n",
    "                pa <- yaux:min((yaux + pool$win_size - 1), dx_h);\n",
    "                for (x in 1:(dim(dy)[4]))\n",
    "                {\n",
    "                    xaux <- x * pool$stride - 1;\n",
    "                    pb <- xaux:min((xaux + pool$win_size - 1), dx_w);\n",
    "                    dx[i, c, pa, pb] <- dx[i, c, pa, pb] + dy[i, c, y, x];\n",
    "                }\n",
    "            }\n",
    "    list(layer = pool, dx = dx);\n",
    "}\n",
    "backward_pool <- cmpfun(backward_pool_orig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Updates the Pool Layer (Does nothing)\n",
    "get_updates_pool <- function(pool, lr) { pool; }\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_pool <- function(pool) { character(0); }\n",
    "gnames_pool <- function(pool) { character(0); }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a pooling layer\n",
    "create_pool <- function(win_size = 3, stride = 2)\n",
    "{\n",
    "    list(win_size = win_size, stride = stride, padding = win_size %/% 2,\n",
    "         pnames = pnames_pool, gnames = gnames_pool, forward = forward_pool,\n",
    "         backward = backward_pool, get_updates = get_updates_pool);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now again, we create a simplistic Driver for that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_pool <- function()\n",
    "{\n",
    "    batch_size <- 1;\n",
    "    n_channels <- 1;\n",
    "    img_shape <- c(5, 5);\n",
    "    win_size <- 3;\n",
    "\n",
    "    x <- sample_normal(c(batch_size, n_channels, img_shape));\n",
    "    layer <- create_pool(win_size = 3, stride = 2);\n",
    "\n",
    "    ok <- check_grad(layer, x);\n",
    "    if (ok)\tprint('Gradient check passed') else print('Gradient check failed');\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening Layer\n",
    "\n",
    "This layer converts the input in N dimensions into a vector of features. It keeps the original dimensions to be able to reconstruct data into the original input shape.\n",
    "\n",
    "Functions for creating and operating Flattening layers:\n",
    "* **forward_flat**: Forward function. TODO - Explanation\n",
    "* **backward_flat**: Backward function. TODO - Explanation\n",
    "* **get_updates_flat**: Flattening layer does not need to update weights and biases, so it does nothing.\n",
    "\n",
    "* **pnames_flat**: Returns the names of the elements in the layer\n",
    "* **gnames_flat**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_flat**: The constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creates a Flat Vector (2D) from a Convolutional Matrix (4D)\n",
    "##   param x : Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "##   returns : Array of shape (batch_size, n_channels * img_height * img_width)\n",
    "##   updates : flat_layer\n",
    "forward_flat <- function(flat, x)\n",
    "{\n",
    "    dims <- dim(x);\n",
    "    flat[[\"shape\"]] <- dims;\n",
    "\n",
    "    batch_size <- dims[1];\n",
    "    flat_dim <- prod(dims[-1]);\n",
    "\n",
    "    y <- array(x,c(batch_size, flat_dim));\n",
    "    list(layer = flat, y = y);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Unflattens a Flat Vector (2D) to a Convolutional Matrix (4D)\n",
    "##   param dy : Array of shape (batch_size, n_channels * img_height * img_width)\n",
    "##   return   : Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "##   updates  : flat_layer (does nothing)\n",
    "backward_flat <- function(flat, dy)\n",
    "{\n",
    "    dx <- array(dy, flat$shape);\n",
    "    list(layer = flat, dx = dx);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Updates the Flat Layer (Does Nothing)\n",
    "get_updates_flat <- function(flat, lr) { flat; }\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_flat <- function(flat) { character(0); }\n",
    "gnames_flat <- function(flat) { character(0); }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a flattened layer\n",
    "create_flat <- function()\n",
    "{\n",
    "    list(pnames = pnames_flat, gnames = gnames_flat, forward = forward_flat,\n",
    "         backward = backward_flat, get_updates = get_updates_flat);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now again, we create a simplistic Driver for that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_flat <- function()\n",
    "{\n",
    "    batch_size <- 2;\n",
    "    n_channels <- 1;\n",
    "    img_shape <- c(5, 5);\n",
    "\n",
    "    x <- sample_normal(c(batch_size, n_channels, img_shape));\n",
    "    layer <- create_flat();\n",
    "\n",
    "    ok <- check_grad(layer, x);\n",
    "    if (ok)\tprint('Gradient check passed') else print('Gradient check failed');\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectifier Layer\n",
    "\n",
    "This layer passes data through a $max(input, 0)$ function, setting 0 as floor value.\n",
    "\n",
    "Functions for creating and operating Rectifier layers:\n",
    "* **forward_relu**: Forward function. Applies the $max(input, 0)$ function\n",
    "* **backward_relu**: Backward function. Returns the output whether the original input was > 0, or 0 otherwise.\n",
    "* **get_updates_relu**: Rectifier layer does not need to update weights and biases, so it does nothing.\n",
    "\n",
    "* **pnames_relu**: Returns the names of the elements in the layer\n",
    "* **gnames_relu**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_relu**: The constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forwards x by setting max_0\n",
    "##  param x : Array\n",
    "##  returns : Array applied max_0\n",
    "##  updates : relu_layer\n",
    "forward_relu <- function(relu, x)\n",
    "{\n",
    "    x[x < 0] <- 0;\n",
    "    relu[[\"a\"]] <- x;\n",
    "    list(layer = relu, y = x);\n",
    "}\n",
    "\n",
    "## Returns a value activated\n",
    "##  param dy : Array\n",
    "##  return   : Array passed through (max_0)\n",
    "##  updates  : relu_layer (does nothing)\n",
    "backward_relu <- function(relu, dy)\n",
    "{\n",
    "    dx <- dy * as.numeric(relu$a > 0);\n",
    "    list(layer = relu, dx = dx);\n",
    "}\n",
    "\n",
    "## Updates the ReLU Layer (Does Nothing)\n",
    "get_updates_relu <- function(relu, lr) { relu; }\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_relu <- function(relu) { character(0); }\n",
    "gnames_relu <- function(relu) { character(0); }\n",
    "\n",
    "## Returns a ReLU layer\n",
    "create_relu <- function()\n",
    "{\n",
    "    list(pnames = pnames_relu, gnames = gnames_relu, forward = forward_relu,\n",
    "         backward = backward_relu, get_updates = get_updates_relu);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This layer behaves as a regular _FFANN_ or _MLP_ linear layer, by learning from a vector of inputs, returning a vector of _h_ outputs from _h_ hidden units.\n",
    "\n",
    "Functions for creating and operating Linear layers:\n",
    "* **forward_line**: Forward function. TODO - Explanation\n",
    "* **backward_line**: Backward function. TODO - Explanation\n",
    "* **get_updates_line**: Update the weights and bias. TODO - Explanation\n",
    "\n",
    "* **pnames_line**: Returns the names of the elements in the layer\n",
    "* **gnames_line**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_line**: The constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forward for a linear layer\n",
    "##  param x : Numeric vector <n_visible>\n",
    "##  returns : Numeric vector <n_hidden>\n",
    "##  updates : linear_layer\n",
    "forward_line <- function(line, x)\n",
    "{\n",
    "    line[[\"x\"]] <- x;\n",
    "    y <- (x %*% t(line$W)) %+% as.vector(line$b);\n",
    "\n",
    "    list(layer = line, y = y);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Backpropagation for a linear layer\n",
    "##  param dy : Numeric vector <n_hidden>\n",
    "##  returns  : Numeric vector <n_visible>\n",
    "##  updates  : linear_layer\n",
    "backward_line <- function(line, dy)\n",
    "{\n",
    "    dx <- dy %*% line$W;\n",
    "\n",
    "    line[[\"grad_W\"]] <- t(dy) %*% line$x\n",
    "    line[[\"grad_b\"]] <- array(colSums(dy),c(1,ncol(dy)));\n",
    "\n",
    "    list(layer = line, dx = dx);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Updates the Linear Layer\n",
    "get_updates_line <- function(line, lr)\n",
    "{\n",
    "    line[[\"W\"]] = line$W - line$grad_W * lr;\n",
    "    line[[\"b\"]] = line$b - line$grad_b * lr;\n",
    "    line;\n",
    "}\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_line <- function(line) { c(\"W\",\"b\"); }\n",
    "gnames_line <- function(line) { c(\"grad_W\",\"grad_b\"); }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a linear layer\n",
    "create_line <- function(n_visible = 4, n_hidden = 10, scale = 0.01)\n",
    "{   \n",
    "    W <- scale * sample_normal(c(n_hidden, n_visible));\n",
    "    b <- array(0, c(1, n_hidden));\n",
    "\n",
    "    list(W = W, b = b, n_visible = n_visible, n_hidden = n_hidden,\n",
    "         pnames = pnames_line, gnames = gnames_line, forward = forward_line,\n",
    "         backward = backward_line, get_updates = get_updates_line);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now again, we create a simplistic Driver for that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_line <- function()\n",
    "{\n",
    "    batch_size <- 2;\n",
    "    img_shape <- 100;\n",
    "\n",
    "    x <- sample_normal(c(batch_size, img_shape));\n",
    "    layer <- create_line(n_visible = 100, n_hidden = 64, scale = 0.01);\n",
    "\n",
    "    ok <- check_grad(layer, x);\n",
    "    if (ok)\tprint('Gradient check passed') else print('Gradient check failed');\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer\n",
    "\n",
    "This layer computes the SoftMax for each input, for future classification considering the output as probabilities for each classification class.\n",
    "\n",
    "Functions for creating and operating Softmax layers:\n",
    "* **forward_soft**: Forward function. TODO - Explanation\n",
    "* **backward_soft**: Backward function. TODO - Explanation\n",
    "* **get_updates_soft**: Softmax layer does not need to update weights and biases, so it does nothing.\n",
    "\n",
    "* **pnames_soft**: Returns the names of the elements in the layer\n",
    "* **gnames_soft**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_soft**: The constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forward through a softmax function\n",
    "##  param x : Numeric vector <n_visible>\n",
    "##  returns : Numeric vector <n_hidden>\n",
    "##  updates : softmax_layer\n",
    "forward_soft <- function(soft, x)\n",
    "{\n",
    "    soft[[\"a\"]] <- exp(x) / rowSums(exp(x));\n",
    "    list(layer = soft, y = soft$a);\n",
    "}\n",
    "\n",
    "## Backward through the softmax layer\n",
    "##  param x : Numeric vector <n_hidden>\n",
    "##  returns : Numeric vector <n_visible>\n",
    "backward_soft <- function(soft, dy)\n",
    "{\n",
    "    dx <- dy; # Passes dy back\n",
    "    list(layer = soft, dx = dx);\n",
    "}\n",
    "\n",
    "## Updates the SoftMax Layer (Does Nothing)\n",
    "get_updates_soft <- function(soft, lr) { soft; }\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_soft <- function(soft) { character(0); }\n",
    "gnames_soft <- function(soft) { character(0); }\n",
    "\n",
    "## Returns a SoftMax layer\n",
    "create_soft <- function()\n",
    "{\n",
    "    list(pnames = pnames_soft, gnames = gnames_soft, forward = forward_soft,\n",
    "         backward = backward_soft, get_updates = get_updates_soft);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Layer\n",
    "\n",
    "This layer computes the sigmoid for each input, for future classification considering the output as probabilities for each classification class.\n",
    "\n",
    "Functions for creating and operating Softmax layers:\n",
    "* **forward_sigm**: Forward function. TODO - Explanation\n",
    "* **backward_sigm**: Backward function. TODO - Explanation\n",
    "* **get_updates_sigm**: Sigmoid layer does not need to update weights and biases, so it does nothing.\n",
    "\n",
    "* **pnames_sigm**: Returns the names of the elements in the layer\n",
    "* **gnames_sigm**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_sigm**: The constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forward through a sigmoid function\n",
    "##   param x : Numeric vector <n_visible>\n",
    "##   returns : Numeric vector <n_hidden>\n",
    "##   updates : sigmoid_layer\n",
    "forward_sigm <- function(sigm, x)\n",
    "{\n",
    "    sigm[[\"a\"]] <- sigmoid_func(x);\n",
    "    list(layer = sigm, y = sigm$a);\n",
    "}\n",
    "\n",
    "## Backward through the sigmoid layer\n",
    "##   param x : Numeric vector <n_hidden>\n",
    "##   returns : Numeric vector <n_visible>\n",
    "backward_sigm <- function(sigm, dy)\n",
    "{\n",
    "    dx <- sigm$a * (1 - sigm$a) * dy;\n",
    "    list(layer = sigm, dx = dx);\n",
    "}\n",
    "\n",
    "## Updates the sigmoid Layer (Does Nothing)\n",
    "get_updates_sigm <- function(sigm, lr) { sigm; }\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_sigm <- function(sigm) { character(0); }\n",
    "gnames_sigm <- function(sigm) { character(0); }\n",
    "\n",
    "## Returns a sigmoid layer\n",
    "create_sigm <- function()\n",
    "{\n",
    "    list(pnames = pnames_sigm, gnames = gnames_sigm, forward = forward_sigm,\n",
    "         backward = backward_sigm, get_updates = get_updates_sigm);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss Layer\n",
    "\n",
    "This layer just computes the _loss_ given the input and the real layer. It works as an evaluation layer, and returns the error \"output - target\" as feed-back for backpropagation.\n",
    "\n",
    "Functions for creating and operating Cross-Entropy Loss layers:\n",
    "* **forward_cell**: Forward function. TODO - Explanation\n",
    "* **backward_cell**: Backward function. TODO - Explanation\n",
    "* **get_updates_cell**: Cross-Entropy Loss layer does not need to update weights and biases, so it does nothing.\n",
    "\n",
    "* **pnames_cell**: Returns the names of the elements in the layer\n",
    "* **gnames_cell**: Returns the names of the gradients in the layer\n",
    "\n",
    "* **create_cell**: The constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Computes the cross-entriopy for input and labels\n",
    "##  param x : Numeric vector\n",
    "##  returns : Numeric vector, Loss\n",
    "forward_cell <- function(cell, x, targets)\n",
    "{\n",
    "    l <- -targets * log(x + 1e-08);\n",
    "    l <- mean(apply(l, MARGIN = 1, sum));\n",
    "    list(layer = cell, y = x, loss = l);\n",
    "}\n",
    "\n",
    "## Backpropagation of Cross-Entropy Layer\n",
    "##  param x : Numeric vector\n",
    "##  returns : Numeric vector, Loss\n",
    "backward_cell <- function(cell, dy, targets)\n",
    "{\n",
    "    num_batches <- dim(dy)[1];\n",
    "    dx <- (1.0 / num_batches) * (dy - targets);\n",
    "    list(layer = cell, dx = dx);\n",
    "}\n",
    "\n",
    "## Updates the C-E Loss Layer (Does Nothing)\n",
    "get_updates_cell <- function(cell, lr) { cell; }\n",
    "\n",
    "## Get names of parameters and gradients (for testing functions)\n",
    "pnames_cell <- function(cell) { character(0); }\n",
    "gnames_cell <- function(cell) { character(0); }\n",
    "\n",
    "## Returns a CrossEntropy Loss layer\n",
    "create_cell <- function()\n",
    "{\n",
    "    list(pnames = pnames_cell, gnames = gnames_cell, forward = forward_cell,\n",
    "         backward = backward_cell, get_updates = get_updates_cell);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructor\n",
    "\n",
    "This, knowing that each layer has already its own constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convolutional Neural Network (CNN). Constructor\n",
    "create_cnn <- function(layers, loss_layer)\n",
    "{\n",
    "    list(layers = layers, loss_layer = loss_layer);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train your CNN\n",
    "\n",
    "Functions to train a CNN from a loaded DataSet:\n",
    "\n",
    "* **train_cnn**: Creates and trains a CNN from a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to train the CNN\n",
    "##  param training_x      : loaded dataset (rows = examples, cols = features)\n",
    "##  param training_y      : loaded labels (binarized vector into rows = examples, cols = labels)\n",
    "##  param training_epochs : number of epochs used for training\n",
    "##  param batch_size      : size of a batch used to train the CNN\n",
    "##  param learning_rate   : learning rate used for training the CNN\n",
    "##  param momentum        : momentum rate used for training the CNN (Currently not used)\n",
    "##  param rand_seed       : random seed for training\n",
    "train_cnn <- function ( training_x, training_y, layers, training_epochs = 300,\n",
    "                       batch_size = 4, learning_rate = 1e-4, momentum = NULL,\n",
    "                       rand_seed = 1234)\n",
    "{\n",
    "    set.seed(rand_seed);\n",
    "\n",
    "    num_samples <- nrow(training_x)\n",
    "    num_batches <- num_samples %/% batch_size;\n",
    "\n",
    "    loss_layer <- create_cell();\n",
    "\n",
    "    for (epoch in 1:training_epochs)\n",
    "    {\n",
    "        start_time <- Sys.time();\n",
    "\n",
    "        acc_loss <- NULL;\n",
    "        for (j in 1:num_batches)\n",
    "        {\n",
    "            # Select mini-batch\n",
    "            idx <- ((j - 1) * batch_size + 1):(j * batch_size);\n",
    "            batchdata <- training_x[idx,,,,drop = FALSE]; # [batch_size x n_channel x img_h x img_w]\n",
    "            targets <- training_y[idx];                   # [batch_size]\n",
    "\n",
    "            # Forward\n",
    "            for (i in 1:length(layers))\n",
    "            {\n",
    "                layer <- layers[[i]];\n",
    "                forward <- layer$forward;\n",
    "\n",
    "                aux <- forward(layer, batchdata);\n",
    "\n",
    "                layers[[i]] <- aux$layer;\n",
    "                batchdata <- aux$y;\n",
    "            }\n",
    "            output <- batchdata;\n",
    "\n",
    "            # Calculate Forward Loss\n",
    "            aux <- loss_layer$forward(loss_layer, output, targets);\n",
    "            loss_layer <- aux$layer;\n",
    "            loss <- aux$loss;\n",
    "\n",
    "            # Calculate negdata\n",
    "            aux <- loss_layer$backward(loss_layer, output, targets);\n",
    "            loss_layer <- aux$layer;\n",
    "            negdata <- aux$dx;\n",
    "\n",
    "            # Backward\n",
    "            for (i in length(layers):1)\n",
    "            {\n",
    "                layer <- layers[[i]];\n",
    "                backward <- layer$backward;\n",
    "\n",
    "                aux <- backward(layer, negdata);\n",
    "\n",
    "                layers[[i]] <- aux$layer;\n",
    "                negdata <- aux$dx;\n",
    "            }\n",
    "\n",
    "            # Update layers\n",
    "            for (i in 1:length(layers))\n",
    "            {\n",
    "                layer <- layers[[i]];\n",
    "                get_updates <- layer$get_updates;\n",
    "\n",
    "                layers[[i]] <- get_updates(layer, learning_rate);\n",
    "            }\n",
    "            acc_loss <- c(acc_loss, loss);\n",
    "        }\n",
    "        end_time <- Sys.time();\n",
    "        \n",
    "        if (epoch %% 1 == 0)\n",
    "            message(paste(\"Epoch\", epoch, \": Mean Loss\", mean(acc_loss, na.rm = TRUE), \"and took\", difftime(end_time, start_time, units = \"mins\"), sep = \" \"));\n",
    "    }\n",
    "\n",
    "    list(create_cnn(layers, loss_layer), loss = mean(acc_loss, na.rm = TRUE));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Values\n",
    "\n",
    "Functions to predict a sequence from a CNN:\n",
    "\n",
    "* **predict_cnn**: ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example: the MNIST dataset\n",
    "\n",
    "The MNIST is a classic dataset with handwritten digits for recognition, by Yann LeCun [here](http://yann.lecun.com/exdb/mnist/). This dataset comes with a training set and testing set, with 60000 and 10000 digits respectively, and their respective vectors indicating the true digit. Each digit is a 784 vector (forming a 28 x 28 image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to load the original dataset file from LeCun's [MNIST digit recognition dataset](http://yann.lecun.com/exdb/mnist/), you will find 4 files (training, test x data, labels). We have previously converted the dataset into a **mnist.rds**, already loaded and prepared.\n",
    "\n",
    "#### Normalization of data\n",
    "\n",
    "Note that after loading the MNIST, data must be normalized, with $min = 0$ and $max = 255$\n",
    "\n",
    "$$X = (X - min) / (max - min) \\rightarrow X = (X - 0) / (255 - 0) \\rightarrow X = X / 255$$\n",
    "\n",
    "Don't forget to divide the inputs by 255 after loading the RDS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist <- readRDS(\"../datasets/mnist.rds\");\n",
    "img_size <- c(28,28);\n",
    "\n",
    "# Set up Data as 4D matrix (batch_size, channels, H, W)\n",
    "train <- mnist$train;\n",
    "training_x <- array(train$x, c(nrow(train$x), 1, img_size)) / 255;\n",
    "training_y <- binarization(train$y);\n",
    "\n",
    "test <- mnist$test;\n",
    "testing_x <- array(test$x, c(nrow(test$x), 1, img_size)) / 255;\n",
    "testing_y <- binarization(test$y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing the Convolutional MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers <- list(\n",
    "    create_conv(n_channels = 1, n_filters = 4, filter_size = 5, scale = 0.1),\n",
    "    create_pool(win_size = 3, stride = 2),\n",
    "    create_relu(),\n",
    "    create_conv(n_channels = 4, n_filters = 16, filter_size = 5, scale = 0.1),\n",
    "    create_pool(win_size = 3, stride = 2),\n",
    "    create_relu(),\n",
    "    create_flat(),\n",
    "    create_line(n_visible = 784, n_hidden = 64, scale = 0.01),\n",
    "    create_relu(),\n",
    "    create_line(n_visible = 64, n_hidden = 10, scale = 0.1),\n",
    "    create_soft()\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a CNN to learn MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Mean Loss 2.53286626627922 and took 1.11678899526596\n",
      "Epoch 2 : Mean Loss 2.53286583379778 and took 1.10601244370143\n",
      "Epoch 3 : Mean Loss 2.53286673101565 and took 1.05864324172338\n"
     ]
    }
   ],
   "source": [
    "cnn <- train_cnn(training_x[1:2000,,,,drop = FALSE],\n",
    "                 training_y[1:2000,,drop = FALSE],\n",
    "                 layers,\n",
    "                 training_epochs = 3,\n",
    "                 batch_size = 10,\n",
    "                 learning_rate = 5e-4\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
