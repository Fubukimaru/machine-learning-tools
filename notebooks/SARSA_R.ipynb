{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "**Temporal Differencing** Q-Learning formula:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha (R(s) + \\gamma \\cdot max_{a'} Q(s', a') - Q(s, a))$$\n",
    "\n",
    "...also known as SARSA when the new state selection is obtained from our current $Q(s,a)$ look-up table.\n",
    "\n",
    " * $Q(s,a)$ : is the **score** of the current status given the current action\n",
    " * $\\alpha$ : is the **learning rate**\n",
    " * $R(s)$ : is the **reward** for reaching status *s*\n",
    " * $\\gamma$ : is the **discount factor** for the expected new score from next status\n",
    " * $max_{a'} Q(s',a')$ : is the **best expected score** for available actions and status\n",
    " \n",
    "We also consider the Utility Function $U(s)$ as\n",
    " \n",
    "$$U(s) = max_{a} Q(s,a) $$\n",
    " \n",
    "so we want at each step maximize the *Utility* of our state by choosing the best action each time:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha (R(s) + \\gamma \\cdot U(s') - Q(s, a))$$\n",
    "\n",
    "When *s* is a final state, we consider the utility as the final reward obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Q-Learning function\n",
    "\n",
    "We create the **qlearn** function, including the **utility** function that will be used (in this case, exhaustive for each possible action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qlearn <- function (actions, rewards, s.initial, alpha, gamma, max.iters, q.scoring = NULL)\n",
    "{\n",
    "    # Auxiliar function to get the index of a given action\n",
    "    get.action.index <- function(a)\n",
    "    {\n",
    "        which(sapply(actions, function(x) all(x == a)));\n",
    "    }\n",
    "    \n",
    "    # Utility Function // Fitness Function\n",
    "    u.function <- function(s, valid.actions)\n",
    "    {\n",
    "        best.q <- -Inf;\n",
    "        best.a <- -Inf;\n",
    "        best.s <- s;\n",
    "        \n",
    "        valid.actions <- sample(valid.actions);\n",
    "        for (a.prime in valid.actions)\n",
    "        {\n",
    "            # Tentative Status\n",
    "            s.prime <- a.prime + s;\n",
    "\n",
    "            # Check score for Q(state', action')\n",
    "            q.prime <- q.scoring[s.prime[1], s.prime[2], get.action.index(a.prime)];\n",
    "            \n",
    "            if (q.prime > best.q)\n",
    "            {\n",
    "                best.q <- q.prime;\n",
    "                best.a <- a.prime;\n",
    "                best.s <- s.prime;    # ... for not computing best.s again later\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        list(q = best.q, a = best.a, s = best.s);\n",
    "    }\n",
    "\n",
    "    # Viability Function\n",
    "    v.function <- function(s)\n",
    "    {\n",
    "        valid.actions <- list();\n",
    "        for (i in 1:length(actions))\n",
    "        {\n",
    "            a.prime <- actions[[i]];\n",
    "\n",
    "            # Check if current action brings us out of bounds\n",
    "            if (s[1] + a.prime[1] > 0 & s[2] + a.prime[2] > 0 &\n",
    "                s[1] + a.prime[1] <= dim(rewards)[1] & s[2] + a.prime[2] <= dim(rewards)[2])\n",
    "                valid.actions[[length(valid.actions) + 1]] <- a.prime;\n",
    "        }\n",
    "        valid.actions;\n",
    "    }\n",
    "    \n",
    "    # Initialize scoring, state and action variables\n",
    "    if (is.null(q.scoring)) q.scoring <- array(0.5, c(dim(rewards)[1], c(dim(rewards)[2]), length(actions)));\n",
    "    s <- s.initial;\n",
    "    a <- actions[[1]];\n",
    "    \n",
    "    # Initialize counters for breaking loop\n",
    "    convergence.count <- 0;\n",
    "    iteration.count <- 0;\n",
    "    prev.status <- c(-1,-1);    # To detect two-step periods...\n",
    "\n",
    "    while (iteration.count < max.iters)\n",
    "    {\n",
    "        # Get valid actions\n",
    "        valid.actions <- v.function(s);\n",
    "        \n",
    "        # Get maximum Q in current Status S\n",
    "        best <- u.function(s, valid.actions);\n",
    "        \n",
    "        # Check convergence. We add a convergence threshold to stop\n",
    "        convergence.count <- if (all(s == best$s) || (prev.status == best$s)) convergence.count + 1 else 0;\n",
    "        if (convergence.count > 5) break;\n",
    "        \n",
    "        # Solve Reward\n",
    "        r <- rewards[s[1],s[2]];\n",
    "        \n",
    "        # Update State-Action Table\n",
    "        action.index <- get.action.index(a);\n",
    "        current.score <- q.scoring[s[1], s[2], action.index];\n",
    "        q.scoring[s[1], s[2], action.index] <- current.score + alpha * (r + gamma * best$q - current.score);\n",
    "\n",
    "        # Change the Status\n",
    "        prev.status <- s;\n",
    "        s <- best$s;\n",
    "        a <- best$a;   \n",
    "        iteration.count <- iteration.count + 1;\n",
    "\n",
    "        message(paste(\"Iteration: \", iteration.count,\n",
    "                      \" Best Position: \", paste(best$s, collapse = \",\"),\n",
    "                      \" Best Action: \", paste(best$a, collapse = \",\"),\n",
    "                      \" Best Value: \", best$q, sep = \"\")\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    list(Final.Status = s,\n",
    "         Final.Action = a,\n",
    "         Final.Reward = rewards[s[1],s[2]],\n",
    "         Iterations = iteration.count,\n",
    "         Final.Score = q.scoring\n",
    "    );\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case of Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a bidimensional space of 4 x 4 cells, with the following rewards for being in each cell:\n",
    "\n",
    "|          | [,1] | [,2] |  [,3] | [,4] |\n",
    "|:--------:|:----:|:----:|:-----:|:----:|\n",
    "| **[1,]** |  +0  |  +0  |    +0 |   +0 |\n",
    "| **[2,]** |  +0  |  +0  |    +0 |   +0 |\n",
    "| **[3,]** |  +0  |  +0  | -0.04 |   +0 |\n",
    "| **[4,]** |  +0  |  +0  |    +1 | -0.1 |\n",
    "\n",
    "There is a goal point, position **[4,3]**, with high reward for being on it, and no reward or negative reward for leaving it.\n",
    "\n",
    "Our actions are *king* movements in a chess game, plus the *No Operation* movement. Adding the NOP movement allows us to remain in the best position when found, then exhaust the convergence steps until loop breaks, finishing the game. The NOP has as drawback that we could get stuck in a local sub-optimal, while forcing us to always move could let us escape from them.\n",
    "\n",
    "Problem Details:\n",
    "\n",
    "* Space has dimensions 4 x 4\n",
    "* Goal is to reach [4,3] (We don't tell which is the goal, but we reward it better)\n",
    "* Start point is at Random\n",
    "* Reward depends only on the current position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Best Position: 1,4 Best Action: -1,0 Best Value: 0.5\n",
      "Iteration: 2 Best Position: 2,4 Best Action: 1,0 Best Value: 0.5\n",
      "Iteration: 3 Best Position: 2,3 Best Action: 0,-1 Best Value: 0.5\n",
      "Iteration: 4 Best Position: 2,4 Best Action: 0,1 Best Value: 0.5\n",
      "Iteration: 5 Best Position: 2,4 Best Action: 0,0 Best Value: 0.5\n",
      "Iteration: 6 Best Position: 2,4 Best Action: 0,0 Best Value: 0.5\n",
      "Iteration: 7 Best Position: 3,3 Best Action: 1,-1 Best Value: 0.5\n",
      "Iteration: 8 Best Position: 4,3 Best Action: 1,0 Best Value: 0.5\n",
      "Iteration: 9 Best Position: 3,4 Best Action: -1,1 Best Value: 0.5\n",
      "Iteration: 10 Best Position: 4,3 Best Action: 1,-1 Best Value: 0.5\n",
      "Iteration: 11 Best Position: 3,2 Best Action: -1,-1 Best Value: 0.5\n",
      "Iteration: 12 Best Position: 4,1 Best Action: 1,-1 Best Value: 0.5\n",
      "Iteration: 13 Best Position: 3,2 Best Action: -1,1 Best Value: 0.5\n",
      "Iteration: 14 Best Position: 2,3 Best Action: -1,1 Best Value: 0.5\n",
      "Iteration: 15 Best Position: 2,3 Best Action: 0,0 Best Value: 0.5\n",
      "Iteration: 16 Best Position: 1,4 Best Action: -1,1 Best Value: 0.5\n",
      "Iteration: 17 Best Position: 1,3 Best Action: 0,-1 Best Value: 0.5\n",
      "Iteration: 18 Best Position: 1,3 Best Action: 0,0 Best Value: 0.5\n",
      "Iteration: 19 Best Position: 2,3 Best Action: 1,0 Best Value: 0.5\n",
      "Iteration: 20 Best Position: 3,4 Best Action: 1,1 Best Value: 0.5\n",
      "Iteration: 21 Best Position: 4,3 Best Action: 1,-1 Best Value: 1\n",
      "Iteration: 22 Best Position: 3,2 Best Action: -1,-1 Best Value: 0.5\n",
      "Iteration: 23 Best Position: 4,1 Best Action: 1,-1 Best Value: 0.5\n",
      "Iteration: 24 Best Position: 3,1 Best Action: -1,0 Best Value: 0.5\n",
      "Iteration: 25 Best Position: 2,2 Best Action: -1,1 Best Value: 0.5\n",
      "Iteration: 26 Best Position: 2,2 Best Action: 0,0 Best Value: 0.5\n",
      "Iteration: 27 Best Position: 3,1 Best Action: 1,-1 Best Value: 0.5\n",
      "Iteration: 28 Best Position: 3,1 Best Action: 0,0 Best Value: 0.5\n",
      "Iteration: 29 Best Position: 4,1 Best Action: 1,0 Best Value: 0.5\n",
      "Iteration: 30 Best Position: 4,2 Best Action: 0,1 Best Value: 0.5\n",
      "Iteration: 31 Best Position: 3,2 Best Action: -1,0 Best Value: 0.5\n",
      "Iteration: 32 Best Position: 2,3 Best Action: -1,1 Best Value: 0.5\n",
      "Iteration: 33 Best Position: 3,4 Best Action: 1,1 Best Value: 0.75\n",
      "Iteration: 34 Best Position: 4,3 Best Action: 1,-1 Best Value: 1.25\n",
      "Iteration: 35 Best Position: 3,2 Best Action: -1,-1 Best Value: 0.5\n",
      "Iteration: 36 Best Position: 2,3 Best Action: -1,1 Best Value: 0.625\n",
      "Iteration: 37 Best Position: 3,4 Best Action: 1,1 Best Value: 1\n",
      "Iteration: 38 Best Position: 4,3 Best Action: 1,-1 Best Value: 1.375\n",
      "Iteration: 39 Best Position: 3,2 Best Action: -1,-1 Best Value: 0.5625\n",
      "Iteration: 40 Best Position: 2,3 Best Action: -1,1 Best Value: 0.8125\n",
      "Iteration: 41 Best Position: 3,4 Best Action: 1,1 Best Value: 1.1875\n",
      "Iteration: 42 Best Position: 4,3 Best Action: 1,-1 Best Value: 1.46875\n",
      "Iteration: 43 Best Position: 3,2 Best Action: -1,-1 Best Value: 0.6875\n",
      "Iteration: 44 Best Position: 2,3 Best Action: -1,1 Best Value: 1\n",
      "Iteration: 45 Best Position: 3,4 Best Action: 1,1 Best Value: 1.328125\n",
      "Iteration: 46 Best Position: 4,3 Best Action: 1,-1 Best Value: 1.578125\n",
      "Iteration: 47 Best Position: 3,2 Best Action: -1,-1 Best Value: 0.84375\n",
      "Iteration: 48 Best Position: 2,3 Best Action: -1,1 Best Value: 1.1640625\n",
      "Iteration: 49 Best Position: 3,4 Best Action: 1,1 Best Value: 1.453125\n",
      "Iteration: 50 Best Position: 4,3 Best Action: 1,-1 Best Value: 1.7109375\n"
     ]
    }
   ],
   "source": [
    "actions <- list(c(0,1), c(1,1), c(1,0), c(1,-1), c(0,-1), c(-1,-1), c(-1,0), c(-1,1), c(0,0));\n",
    "\n",
    "rewards <- matrix(0, ncol = 4, nrow = 4);\n",
    "rewards[3,3] <- -0.04;\n",
    "rewards[4,3] <- +1;\n",
    "rewards[4,4] <- -0.1;\n",
    "\n",
    "s.initial <- c(sample(1:4,1), sample(1:4,1));\n",
    "\n",
    "result <- qlearn(actions, rewards, s.initial, alpha = 0.5, gamma = 1.0, max.iters = 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 5\n",
      " $ Final.Status: num [1:2] 4 3\n",
      " $ Final.Action: num [1:2] 1 -1\n",
      " $ Final.Reward: num 1\n",
      " $ Iterations  : num 50\n",
      " $ Final.Score : num [1:4, 1:4, 1:9] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n"
     ]
    }
   ],
   "source": [
    "str(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
